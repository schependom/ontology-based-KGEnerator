# Ontology-based Data Generator for KGE Models

A backward-chaining, ontology-based data generator for Knowledge Graph Embedding (KGE) models. This tool generates synthetic Knowledge Graphs (KGs) that are consistent with a given OWL 2 RL ontology, providing ground-truth explanations (proof trees) for every inferred fact.

## Project Overview & Technical Logic

This generator addresses the challenge of evaluating KGE models on reasoning tasks by creating datasets where the "ground truth" logic is fully known. Unlike real-world KGs where inferences are implicit or noisy, this generator constructs KGs *from* the logic itself.

### Mathematical Formulation

Let $\Sigma = (\mathcal{S}, \mathcal{R})$ be an ontology, where $\mathcal{S}$ is the schema (classes, relations) and $\mathcal{R}$ is a set of Horn-like rules. A rule $r \in \mathcal{R}$ has the form:
$$ \text{body}_1 \land \text{body}_2 \land \dots \land \text{body}_n \implies \text{head} $$

The generation process $\mathcal{G}(\Sigma) \to \mathcal{K}$ follows these steps:

1.  **Goal Sampling**: A subset of rules $R_{sub} \subseteq \mathcal{R}$ is selected to serve as goals.
2.  **Backward Chaining**: For each goal rule, the engine performs backward chaining to find valid **Proof Trees** ($\pi$).
    *   A proof tree $\pi$ has a root node (the inferred fact) and leaf nodes (base facts).
    *   Internal nodes represent intermediate derivation steps.
    *   The depth of the tree is bounded by `max_recursion` and `global_max_depth` to prevent infinite loops in cyclic ontologies.
3.  **Variable Instantiation**: The abstract proof trees contain variables. These are instantiated with individuals from a pool $\mathcal{I}$.
    *   **Individual Reuse**: To create dense, connected graphs, individuals are reused across different facts with probability $P_{reuse}$.
4.  **Graph Construction**:
    *   $\mathcal{K}^+_{base}$: The set of all leaf nodes (base facts).
    *   $\mathcal{K}^+_{inferred}$: The set of all root and internal nodes (inferred facts).
    *   $\mathcal{K}^+ = \mathcal{K}^+_{base} \cup \mathcal{K}^+_{inferred}$.
5.  **Negative Sampling**:
    *   Negative facts $\mathcal{K}^-$ are generated by corrupting valid triples $(s, p, o) \in \mathcal{K}^+$.

### Types of Negative Sampling

1. **Random Corruption**: 
    - Randomly corrupts valid triples.
    - E.g. $(\mathrm{Ind}_0, \mathrm{hasMother}, \mathrm{Ind}_1)$, where $\mathrm{Ind}_1$ could be any individual, and thus not necessarily a woman, even though the range of `hasMother` is `Woman`. 
2. **Type-constrained Corruption**: 
    - Corrupts valid triples while preserving the type of the head.
    - E.g. $(\mathrm{Ind}_0, \mathrm{hasMother}, \mathrm{Ind}_1)$, where $\mathrm{Ind}_1$ must be a woman, because the range of `hasMother` is `Woman`.
3. **Proof-based Corruption**: 
    - Corrupts valid triples based on the proof tree.
    - If a base fact in a proof tree is corrupted, the corruption propagates up the tree, creating "hard" negative inferred facts that look structurally valid but are false.
    - Note that this new negative fact can cause inconsistencies in the graph. If this is the case, ... TODO ...
    - E.g. $(X, \mathrm{hasFather}, Y) \land (Y, \mathrm{hasFather}, Z) \implies (X, \mathrm{hasGrandfather}, Z)$
        - Corrupt base fact $(X, \mathrm{hasFather}, Y)$
        - The corruption propagates up the tree, creating $(X, \lnot\mathrm{hasGrandfather}, Z)$.
        - If an inconsistency is detected, ... TODO ...


## Installation

Create a Python environment (tested with Python 3.11):

```bash
conda env create -f environment.yaml
conda activate KGEnerator
pip install -r requirements.txt
```

Note: The `graphviz` Python package requires the Graphviz system binary to render graphs. On macOS you can install it with:

```bash
brew install graphviz
```

## Usage

All scripts should be run from the project root.

### 1. Generating Balanced Train/Test Data for RRN

To generate a dataset suitable for training a Relational Reasoning Network (RRN) or similar neuro-symbolic models, you typically want:
*   Multiple independent small graphs (samples).
*   A balanced mix of positive and negative examples (`--neg-ratio 1.0`).
*   A mix of negative sampling strategies to encourage robust learning.

```bash
python src/create_data.py \
    --ontology-path data/family.ttl \
    --output data/rrn_dataset \
    --n-train 10 \
    --n-test 10 \
    --min-individuals 10 \
    --max-individuals 500 \
    --neg-ratio 1.0 \
    --neg-strategy mixed \
    --export-graphs
```

*   `--neg-strategy mixed`: Uses a combination of random, type-constrained, and proof-based corruption.
*   `--export-graphs`: (Optional) Saves visualizations of a few samples for inspection.

### 2. Generating One Knowledge Graph

In order to manually (and visually) verify that the proofs, facts and negative examples are correct, we can generate a single Knowledge Graph that attempts to contain **all possible facts** and **all possible proofs** derivable from a toy ontology (within finite bounds).

```bash
python ... TODO ... --ontology-path data/toy.ttl ... TODO ...
```

TODO
