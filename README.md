# Ontology-based Data Generator for Neuro-Symbolic AI

A backward-chaining, ontology-based synthetic data generator designed for training and evaluating Neuro-Symbolic AI (NeSy) and Knowledge Graph Embedding (KGE) models.

This tool generates ground-truth Knowledge Graphs (KGs) from OWL 2 RL ontologies, providing **reasoning-aware** datasets where every inferred fact is backed by a complete **proof tree**.

## Key Features

1.  **Inductive Split Generation**: Generates completely independent Training and Testing graphs with disjoint entity sets (no shared individuals), simulating true inductive learning scenarios.
    *   Training entities: `train_0`, `train_1`, ...
    *   Testing entities: `test_0`, `test_1`, ...
2.  **Explainable Negative Sampling**: Generates "Hard Negatives" by corrupting reasoning chains.
    *   **Explainability**: Exports a CSV (`negatives_explanations.csv`) detailing exactly *why* a negative is false (e.g., "Corrupted Base Fact in proof for X via Rule Y").
3.  **Graph Topology Control**: Configurable parameters to control graph density, connectivity, and diameter to match real-world benchmarks.
4.  **Full Reasoning Provenance**: Every inferred fact comes with a materialized proof tree, allowing for simplified "Reasoning-as-Retrieval" training.

## Mathematical Formulation

Let $\Sigma = (\mathcal{S}, \mathcal{R})$ be an ontology, where $\mathcal{S}$ is the schema (classes, relations) and $\mathcal{R}$ is a set of Horn-like rules. A rule $r \in \mathcal{R}$ has the form:

$$\text{body}_1 \land \text{body}_2 \land \dots \land \text{body}_n \implies \text{head}$$

The generation process $\mathcal{G}(\Sigma) \to \mathcal{K}$ follows these steps:

1.  **Goal Sampling**: A subset of rules $R_{sub} \subseteq \mathcal{R}$ is selected to serve as goals.
2.  **Backward Chaining**: For each goal rule, the engine performs backward chaining to find valid **Proof Trees** ($\pi$).
    *   A proof tree $\pi$ has a root node (the inferred fact) and leaf nodes (base facts).
    *   Internal nodes represent intermediate derivation steps.
    *   The depth of the tree is bounded by `max_recursion` and `global_max_depth` to prevent infinite loops in cyclic ontologies.
3.  **Variable Instantiation**: The abstract proof trees contain variables. These are instantiated with individuals from a pool $\mathcal{I}$.
    *   **Inductive Split**: $\mathcal{I}$ is partitioned into disjoint sets $\mathcal{I}_{train}$ and $\mathcal{I}_{test}$ to prevent entity leakage.
    *   **Individual Reuse**: To create dense, connected graphs, individuals are reused across different facts with probability $P_{reuse}$.
4.  **Graph Construction**:
    *   $\mathcal{K}^+_{base}$: The set of all leaf nodes (base facts).
    *   $\mathcal{K}^+_{inferred}$: The set of all root and internal nodes (inferred facts).
    *   $\mathcal{K}^+ = \mathcal{K}^+_{base} \cup \mathcal{K}^+_{inferred}$.
5.  **Negative Sampling & Explainability**:
    *   Negative facts $\mathcal{K}^-$ are generated by corrupting valid triples $(s, p, o) \in \mathcal{K}^+$.
    *   **Explanation**: For proof-based corruption, we record an explanation $\epsilon$ describing the specific corruption in the proof tree (e.g., base fact modification) that breaks the inference chain.


## Installation

Create a Python environment (tested with Python 3.11):

```bash
conda env create -f environment.yaml
conda activate KGEnerator
pip install -r requirements.txt
```

*Note: `graphviz` is required for proof visualization.*
```bash
brew install graphviz  # macOS
sudo apt-get install graphviz # Ubuntu
```

## Quick Start

### 1. Generate Inductive Train/Test Data
This command generates a dataset suitable for training models like RRN (Reasoning-Relation Networks).

```bash
python src/create_data.py \
    --ontology-path data/family.ttl \
    --output data/output_v1 \
    --n-train 100 \
    --n-test 20 \
    --min-individuals 10 \
    --max-individuals 50 \
    --neg-strategy proof_based \
    --neg-ratio 1.0 \
    --verbose
```

**Output Structure:**
```
data/output_v1/
├── train/
│   ├── train_sample_00000.csv       # Training Graph 0
│   ├── train_sample_00001.csv       # Training Graph 1
│   └── negatives_explanations.csv   # Explanations for hard negatives
└── test/
    ├── test_sample_00000.csv        # Test Graph 0 (Disjoint entities)
    └── negatives_explanations.csv
```

### 2. Generate a Single "Toy" Graph for Inspection
Useful for debugging ontology rules or visualizing proof trees.

```bash
python src/generate.py \
    --ontology-path data/toy.ttl \
    --neg-strategy proof_based \
    --export-proofs \
    --verbose
```

## Detailed Usage

### `create_data.py` Arguments

| Argument | Default | Description |
|:---|:---|:---|
| `--ontology-path` | **Required** | Path to the `.ttl` ontology file. |
| `--output` | `data/out/` | Directory to save generated datasets. |
| `--n-train` | `5` | Number of training graphs to generate. |
| `--n-test` | `2` | Number of test graphs to generate. |
| `--min-individuals` | `1` | Min individuals per graph. |
| `--max-individuals` | `1000` | Max individuals per graph. |
| `--neg-strategy` | `mixed` | Strategy: `random`, `constrained`, `proof_based`, `type_aware`, `mixed`. |
| `--neg-corrupt-base-facts` | `False` | For `proof_based`: corrupts leaves to falsify roots. |
| `--export-proofs` | `False` | Export PDF visualizations of proof trees. |

## Negative Sampling Strategies

1.  **Random**: Replaces subject/object with any random individual.
    *   *Weakness*: Easy to detect via type checking (e.g., `hasMother` -> `Table`).
2.  **Constrained**: Respects Domain/Range.
    *   *Strength*: Semantically valid, harder to detect.
3.  **Type-Aware**: Substitutes with individuals of the exact same class set.
    *   *Strength*: Very hard to detect without relational reasoning.
4.  **Proof-Based** (Advanced):
    *   Takes a valid proof tree: `(A, parent, B) ^ (B, parent, C) -> (A, grandparent, C)`.
    *   Corrupts a base fact: `(A, parent, D)`.
    *   Propagates change to assert `(A, grandparent, C)` (which is now likely false).
    *   **Result**: A "Hard Negative" that requires understanding the full rule chain to refute.

## Project Structure

*   `src/chainer.py`: Backward-chaining inference engine.
*   `src/create_data.py`: Main orchestrator for dataset generation.
*   `src/negative_sampler.py`: Implementation of corruption strategies.
*   `src/parser.py`: Parses OWL 2 RL ontologies into executable rules.
*   `src/validator.py`: Checks graphs for consistency constraints (Disjointness, etc.).
*   `data/`: Contains example ontologies (`family.ttl`, `toy.ttl`).

## Data Format

### Sample CSV (`train_sample_XXXX.csv`)
Standard triple format with metadata.
```csv
subject,predicate,object,label,fact_type,is_inferred
train_0,hasParent,train_1,1,triple,0
train_0,hasGrandparent,train_2,1,triple,1
train_0,hasParent,train_99,0,triple,0  <-- Negative Sample
```

### Explanations CSV (`negatives_explanations.csv`)
```csv
sample_id,subject,predicate,object,explanation
sample_0000,train_0,hasGrandparent,train_2,"Propagated from corrupted base fact (train_0 hasParent train_1 -> train_0 hasParent train_99) via Rule grandparent_rule"
```
